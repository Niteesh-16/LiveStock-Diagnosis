{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f90990",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Livestock Disease Prognosis - Cleaned Python Script with SHAP\n",
    "# Consolidated from recurrent-free Jupyter notebook\n",
    "# Run this in a Python environment with required libs: pandas, sklearn, boruta, catboost, xgboost, lime, shap\n",
    "# Assumes 'Data.csv' in same directory\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from boruta import BorutaPy\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import shap  # Added for SHAP\n",
    "\n",
    "# =============================================================================\n",
    "# Data Loading & Preprocessing\n",
    "# =============================================================================\n",
    "with open(\"Data.csv\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "lines = [line.strip().replace('\"', '') for line in lines]\n",
    "data = [line.split(',') for line in lines]\n",
    "\n",
    "df = pd.DataFrame(data[1:], columns=data[0])  \n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(df.columns.tolist())\n",
    "\n",
    "X = df.drop(\"Disease\", axis=1)\n",
    "y = df[\"Disease\"]\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "numeric_features = ['Age', 'Temperature']\n",
    "categorical_features = ['Animal']\n",
    "binary_features = [col for col in X.columns if col not in numeric_features + categorical_features]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), numeric_features),\n",
    "    (\"cat\", OneHotEncoder(), categorical_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "baseline_pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"rf\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "baseline_pipeline.fit(X, y_encoded)\n",
    "\n",
    "onehot_cols = baseline_pipeline.named_steps['preprocess'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "all_features = numeric_features + list(onehot_cols) + binary_features\n",
    "\n",
    "importances = baseline_pipeline.named_steps['rf'].feature_importances_\n",
    "\n",
    "feat_imp = pd.Series(importances, index=all_features).sort_values(ascending=False)\n",
    "top_20_features = feat_imp.head(20).index.tolist()\n",
    "print(\"Top 20 features:\", top_20_features)\n",
    "\n",
    "X_preprocessed = baseline_pipeline.named_steps['preprocess'].transform(X)\n",
    "X_pre_df = pd.DataFrame(X_preprocessed, columns=all_features)\n",
    "X_top20 = X_pre_df[top_20_features]\n",
    "X_top20_cleaned = X_top20.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "print(X_top20_cleaned.dtypes)\n",
    "print(\"Missing values:\\n\", X_top20_cleaned.isnull().sum())\n",
    "\n",
    "# =============================================================================\n",
    "# Boruta Feature Selection\n",
    "# =============================================================================\n",
    "X_np = X_pre_df.values\n",
    "y_np = y_encoded\n",
    "\n",
    "rf_boruta = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=42)\n",
    "boruta_selector = BorutaPy(rf_boruta, n_estimators='auto', verbose=2, random_state=42)\n",
    "boruta_selector.fit(X_np, y_np)\n",
    "\n",
    "boruta_features = X_pre_df.columns[boruta_selector.support_].to_list()\n",
    "print(\"Boruta selected features:\", boruta_features)\n",
    "\n",
    "X_final = X_pre_df[boruta_features].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# Single Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Unique Animals\n",
    "unique_animals = df['Animal'].unique()\n",
    "print(unique_animals)\n",
    "\n",
    "# =============================================================================\n",
    "# Model Tuning & Evaluation\n",
    "# =============================================================================\n",
    "# Tuned RF\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=3, n_jobs=-1, verbose=1)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "rf_tuned = grid_rf.best_estimator_\n",
    "print(\"Best RF parameters:\", grid_rf.best_params_)\n",
    "\n",
    "# Tuned XGBoost\n",
    "params_xgb = {\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "}\n",
    "search_xgb = RandomizedSearchCV(\n",
    "    XGBClassifier(objective='multi:softmax', num_class=len(set(y_encoded)), use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    param_distributions=params_xgb,\n",
    "    n_iter=20,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "search_xgb.fit(X_train, y_train)\n",
    "xgb_tuned = search_xgb.best_estimator_\n",
    "print(\"Best XGB Accuracy:\", search_xgb.best_score_)\n",
    "print(\"Best XGB Params:\", search_xgb.best_params_)\n",
    "\n",
    "# Tuned CatBoost\n",
    "cat_tuned = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "    depth=6,\n",
    "    learning_rate=0.1,\n",
    "    l2_leaf_reg=3,\n",
    "    verbose=0,\n",
    "    random_state=42\n",
    ")\n",
    "cat_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "models = {\n",
    "    \"Tuned Random Forest\": rf_tuned,\n",
    "    \"Tuned XGBoost\": xgb_tuned,\n",
    "    \"Tuned CatBoost\": cat_tuned\n",
    "}\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n=== {name} ===\\nAccuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# =============================================================================\n",
    "# SHAP Analysis (Added: Global Feature Importance on Tuned RF)\n",
    "# =============================================================================\n",
    "explainer = shap.TreeExplainer(rf_tuned)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Handle multi-class (aggregate mean abs SHAP)\n",
    "if isinstance(shap_values, list) and len(shap_values) == len(le.classes_):\n",
    "    # Multi-class: sum abs over classes, mean over samples\n",
    "    shap_vals = np.array([np.abs(sv).mean(axis=0) for sv in shap_values]).sum(axis=0)\n",
    "else:\n",
    "    # Binary/single: abs mean over samples\n",
    "    shap_vals = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "feature_importance_shap = pd.Series(shap_vals, index=X_train.columns).sort_values(ascending=False)\n",
    "print(\"\\nSHAP Feature Importance:\\n\", feature_importance_shap)\n",
    "\n",
    "# =============================================================================\n",
    "# Ensemble Voting Classifier\n",
    "# =============================================================================\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rf_tuned), ('xgb', xgb_tuned), ('cat', cat_tuned)],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred_ens = voting_clf.predict(X_test)\n",
    "print(\"Voting Classifier Accuracy:\", accuracy_score(y_test, y_pred_ens))\n",
    "\n",
    "# =============================================================================\n",
    "# Prediction on New Data\n",
    "# =============================================================================\n",
    "model_features = X_final.columns\n",
    "new_data = {\n",
    "    'blisters on gums': [1],\n",
    "    'blisters on hooves': [1],\n",
    "    'blisters on mouth': [1],\n",
    "    'blisters on tongue': [0],\n",
    "    'chest discomfort': [0],\n",
    "    'chills': [0],\n",
    "    'crackling sound': [0],\n",
    "    'depression': [0]\n",
    "}\n",
    "new_df = pd.DataFrame(0, index=[0], columns=model_features)\n",
    "for symptom, value in new_data.items():\n",
    "    if symptom in new_df.columns:\n",
    "        new_df[symptom] = value[0]\n",
    "\n",
    "predicted_disease = voting_clf.predict(new_df)\n",
    "disease_label = le.classes_[predicted_disease[0]]\n",
    "print(f\"\\nPredicted Disease: {disease_label}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LIME Explanation\n",
    "# =============================================================================\n",
    "X_train_array = X_train.values\n",
    "class_names = list(le.classes_)\n",
    "\n",
    "explainer = LimeTabularExplainer(\n",
    "    training_data=X_train_array,\n",
    "    feature_names=model_features,\n",
    "    class_names=class_names,\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=new_df.iloc[0].values,\n",
    "    predict_fn=voting_clf.predict_proba,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "# Note: exp.show_in_notebook() is Jupyter-specific; in script, use exp.as_list() or save HTML\n",
    "print(exp.as_list())  # Simple text output for script\n",
    "# For full HTML: exp.save_to_file('lime_explanation.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
